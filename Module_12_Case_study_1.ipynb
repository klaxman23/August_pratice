{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO18k4O4LSsLUfElPwD9zPt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/klaxman23/August_pratice/blob/main/Module_12_Case_study_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Case Study – 1\n",
        "Domain – Logistics\n",
        "focus – Optimal path\n",
        "Business challenge/requirement\n",
        "BluEx is a leading logistics company in India. It's known for efficient delivery of\n",
        "packets to customers. However, BluEx is facing a challenge where its van drivers are\n",
        "taking a suboptimal path for delivery. This is causing delays and higher fuel costs.\n",
        "You as an ML expert have to create an ML model using Reinforcement Learning so\n",
        "that an efficient path is found through the program.\n",
        "Key issues\n",
        "Data has lots of attributes and classification could be tricky\n",
        "Considerations\n",
        "Reinforcement Learning is tricky, so the expectation is to come up with a sample flow\n",
        "and full-fledged implementation will be done by the team later\n",
        "Data volume\n",
        "- None. Sample data is hard coded in the program\n",
        "Additional information\n",
        "- NA\n",
        "Business benefits\n",
        "Up to 15% of fuel cost can be saved by taking the optimal path"
      ],
      "metadata": {
        "id": "vgt1em3mz0ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# -----------------------------\n",
        "# Environment Setup\n",
        "# -----------------------------\n",
        "grid_size = 5\n",
        "start = (0, 0)\n",
        "goal = (4, 4)\n",
        "\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "action_map = {\n",
        "    'up': (-1, 0),\n",
        "    'down': (1, 0),\n",
        "    'left': (0, -1),\n",
        "    'right': (0, 1)\n",
        "}\n",
        "\n",
        "# Q-table\n",
        "Q = np.zeros((grid_size, grid_size, len(actions)))\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1      # learning rate\n",
        "gamma = 0.9      # discount factor\n",
        "epsilon = 0.2    # exploration rate\n",
        "episodes = 500\n",
        "\n",
        "# -----------------------------\n",
        "# Training Loop\n",
        "# -----------------------------\n",
        "for episode in range(episodes):\n",
        "    state = start\n",
        "\n",
        "    while state != goal:\n",
        "        x, y = state\n",
        "\n",
        "        # Choose action\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action_index = random.randint(0, 3)\n",
        "        else:\n",
        "            action_index = np.argmax(Q[x, y])\n",
        "\n",
        "        action = actions[action_index]\n",
        "        dx, dy = action_map[action]\n",
        "\n",
        "        new_x, new_y = x + dx, y + dy\n",
        "\n",
        "        # Boundary check\n",
        "        if new_x < 0 or new_x >= grid_size or new_y < 0 or new_y >= grid_size:\n",
        "            reward = -5\n",
        "            new_state = state\n",
        "        elif (new_x, new_y) == goal:\n",
        "            reward = 100\n",
        "            new_state = goal\n",
        "        else:\n",
        "            reward = -1\n",
        "            new_state = (new_x, new_y)\n",
        "\n",
        "        # Q-learning update\n",
        "        Q[x, y, action_index] += alpha * (\n",
        "            reward + gamma * np.max(Q[new_state[0], new_state[1]]) - Q[x, y, action_index]\n",
        "        )\n",
        "\n",
        "        state = new_state\n",
        "\n",
        "# -----------------------------\n",
        "# Optimal Path Extraction\n",
        "# -----------------------------\n",
        "state = start\n",
        "path = [state]\n",
        "\n",
        "while state != goal:\n",
        "    x, y = state\n",
        "    action_index = np.argmax(Q[x, y])\n",
        "    dx, dy = action_map[actions[action_index]]\n",
        "    state = (x + dx, y + dy)\n",
        "    path.append(state)\n",
        "\n",
        "print(\"Optimal Path:\", path)\n"
      ],
      "metadata": {
        "id": "3KBKYgoz0l21"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}